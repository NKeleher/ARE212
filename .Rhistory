legend.key = element_blank(),
panel.grid = element_blank(),
text = element_text(family = "CharisSIL", color = "black",
size = 11),
axis.text = element_text(color = "black"),
title = element_text(size = 12),
legend.text = element_text(size = 11),
legend.key.size = unit(1.5, "cm"))
# My own save functions
save_beamer <- function(gg_tmp, name, width = 16 * 0.7, height = 10 * 0.7,
themes = NULL) {
ggsave(filename = paste0(name, ".pdf"),
path = dir_figures,
plot = gg_tmp + theme_beamer + themes,
width = width,
height = height,
device = cairo_pdf)
embed_fonts(paste0(dir_figures, name, ".pdf"))
}
save_paper <- function(gg_tmp, name, width = 6.4, height = 4,
themes = NULL) {
ggsave(filename = paste0(name, ".pdf"),
path = dir_figures,
plot = gg_tmp + theme_paper + themes,
width = width,
height = height,
device = cairo_pdf)
embed_fonts(paste0(dir_figures, name, ".pdf"))
}
# Load electricity rate data ---------------------------------------------------
# Load the utility-specific datasets
rt_pge <- dir_csv %>% paste0("electricityRatesPGE.csv") %>%
read_csv() %>% data.table()
rt_socal <- dir_csv %>% paste0("electricityRatesSocal.csv") %>%
read_csv() %>% data.table()
# Grab desired variables from SoCal's data
rt_socal <- rt_socal[, c(1:2, 19:23, 18)]
# Convert dates
rt_pge[, `:=`(
date_start = mdy(date_start),
date_stop = mdy(date_stop)
)]
rt_socal[, `:=`(
date_start = mdy(date_start),
date_stop = mdy(date_stop)
)]
# Bind together
rt_dt <- rbindlist(list(rt_pge, rt_socal), fill = T, use.names = T)
# Count the number of days in each rate schedule
rt_dt[, days := (date_stop - date_start + 1) %>% as.numeric()]
# Expand to day level
rt_dt <- rt_dt[rep(seq.int(1, nrow(rt_dt)), rt_dt$days), 1:9]
# Add day-of-rate
rt_dt[, rate_day := seq(1, .N), by = .(utility, date_start)]
# Add day variable
rt_dt[, date := date_start + rate_day - 1]
# Add month, year, and month-year
rt_dt[, `:=`(
mo = month(date),
yr = year(date)
)][, ym := paste0(yr, mo)]
# Keep first of the month
el_dt <- rt_dt[day(date) == 1, .(month_date = date, utility, tier1, tier3)]
# Stack
el_dt <- rbindlist(list(
el_dt[, .(month_date, utility, price = tier1, price_type = "elec_1")],
el_dt[, .(month_date, utility, price = tier3, price_type = "elec_3")]
), use.names = T, fill = T)
# Figure: PG&E and SoCal price regime series -----------------------------------
# Load data
gas_dt <- readRDS(paste0(dir_fig, "ggplotPriceSeries.rds"))$data
# Wide to long
gas_dt <- gas_dt[, .(month_date, charge_base, charge_excess, utility)] %>%
melt(id.var = c("month_date", "utility"),
variable.name = "price_type", value.name = "price")
# Keep the only the base rate
gas_dt <- gas_dt[price_type == "charge_base"]
# Create joint variable of utility and price type
gas_dt[, price_type := "gas"]
# Join gas and electricity data ------------------------------------------------
# Merge datasets for regression
reg_dt <- merge(
x = el_dt, all.x = T,
y = gas_dt[, -"price_type"], all.y = F,
by = c("month_date", "utility"),
suffixes = c("_e", "_g"))
# Stack datasets for plotting
price_dt <- rbindlist(list(gas_dt, el_dt), use.names = T, fill = T)
# Limit to 2010 through 2014
price_dt <- price_dt[year(month_date) %>% between(2010, 2014)]
# Add month, year, and month-year
reg_dt[, `:=`(
mo = month(month_date),
yr = year(month_date)
)][, ym := paste0(yr, mo)]
price_dt[, `:=`(
mo = month(month_date),
yr = year(month_date)
)][, ym := paste0(yr, mo)]
# Create utility by price type
price_dt[, utility_price_type := paste0(utility, "_", price_type)]
# Create utlity comparison dataset for plotting differences
wide_dt <- dcast(price_dt,
month_date ~ utility + price_type, value.var = "price")
# Take within-month, cross-utility differences (pge - socal)
wide_dt[, `:=`(
gas_diff = pge_gas - socal_gas,
elec1_diff = pge_elec_1 - socal_elec_1,
elec3_diff = pge_elec_3 - socal_elec_3
)]
# To long for a differenced data table
diff_dt <- wide_dt[, c(1, 8:10)] %>%
melt(id.var = "month_date", variable.name = "type", value.name = "diff")
# Demean
diff_dt[, diff_dm := diff - mean(diff), by = type]
save_paper(gg_tmp, name = "gasElecSeriesLevels",
width = 9, height = 5.6,
theme(
legend.direction = "horizontal",
legend.title = element_text(face = "bold"),
legend.text = element_text(size = 15),
legend.key.width = unit(1.5, "cm"),
legend.key.height = unit(0.75, "cm"),
NULL)
)
# Plot -------------------------------------------------------------------------
# Time series of levels
gg_tmp <- ggplot(data = price_dt,
aes(x = month_date, y = price,
linetype = utility_price_type,
color = utility_price_type)) +
geom_line() +
ylim(0, 1.3) +
geom_hline(yintercept = 0, color = "grey20", size = 0.4) +
geom_vline(xintercept = min(price_dt$month_date),
color = "grey20", size = 0.4) +
theme_beamer +
ggtitle("Time series of electricity and gas prices for PG&E and SoCal",
subtitle = "Tiers 1 and 3 for electricity; tier 1 for natural gas") +
xlab("Date (month)") +
ylab("Price") +
scale_color_manual("Type of price and utility:",
values = rep(c("#6A1B9A", "#FC845F"), each = 3),
labels = c("PG&E: Elect. Tier 1", "PG&E: Elect. Tier 3", "PG&E: Gas Tier 1",
"SoCal: Elect. Tier 1", "SoCal: Elect. Tier 3", "SoCal: Gas Tier 1")) +
scale_linetype_manual("Type of price and utility:",
values = rep(c("solid", "dotted", "twodash"), 2),
labels = c("PG&E: Elect. Tier 1", "PG&E: Elect. Tier 3", "PG&E: Gas Tier 1",
"SoCal: Elect. Tier 1", "SoCal: Elect. Tier 3", "SoCal: Gas Tier 1")) +
guides(col = guide_legend(byrow = T, nrow = 2))
save_paper(gg_tmp, name = "gasElecSeriesLevels",
width = 9, height = 5.6,
theme(
legend.direction = "horizontal",
legend.title = element_text(face = "bold"),
legend.text = element_text(size = 15),
legend.key.width = unit(1.5, "cm"),
legend.key.height = unit(0.75, "cm"),
NULL)
)
# Setup ------------------------------------------------------------------------
# Options
options(stringsAsFactors = F)
# Packages
library(pacman)
p_load(rmapshaper, sp, rgdal, rgeos, sf, data.table, readr, magrittr)
# Define directories
dir_misc <- "/Users/edwardarubin/Dropbox/Research/MyProjects/Miscellaneous/"
dir_project <- dir_misc %>% paste0("CensusShapefiles/")
dir_raw <- dir_project %>% paste0("DataRaw/")
dir_shp <- dir_project %>% paste0("DataSpatial/")
dir_1950 <- dir_shp %>% paste0("nhgis0024_shapefile_tl2008_us_county_1950/")
us_shp <- readOGR(dsn = dir_1950, layer = "US_county_1950_conflated")
names(us_shp)
us_shp$STATENAM %>% unique
us_shp %<>% subset(STATENAM == "North Carolina")
# Define states and counties ---------------------------------------------------
# Load NC VRA counties
nc_dt <- paste0(dir_raw, "ncVra.csv") %>% read_csv() %>% data.table()
# Change 'McDowell' to 'Mcdowell' to match NHGIS name
nc_dt[county == "McDowell", county := "Mcdowell"]
# Split NC into VR and non-VRA
nc_vra <- nc_dt[covered == 1, county]
nc_non <- nc_dt[covered == 0, county]
# Define the VRA states
vra_1965 <- c("Alabama", "Georgia", "Louisiana", "Mississippi",
"South Carolina", "Virginia")
vra_1975 <- c("Arizona", "Texas")
vra_all <- c(vra_1965, vra_1975)
# Full VRA: Join counties into subsets -----------------------------------------
# NOTE: Treatment starting in 1975
# NOTE: Does note include a few counties in FL, SD, NY, CA
# Create VRA subset; take union
vra_shp <- subset(us_shp,
(STATENAM %in% vra_all) |
((STATENAM == "North Carolina") & (NHGISNAM %in% nc_vra))
) %>% gUnaryUnion()
# Create non-VRA subset
non_shp <- subset(us_shp,
(!(STATENAM %in% c(vra_all, "North Carolina")) |
((STATENAM == "North Carolina") & (NHGISNAM %in% nc_non))
)) %>% gUnaryUnion()
# Take union of all US states
all_shp <- subset(us_shp,
!(STATENAM %in% c("Alaska Territory", "Hawaii Territory"))) %>%
gUnaryUnion()
# Define the VRA boundary
vra_line <- gDifference(
as(vra_shp, "SpatialLines"),
as(all_shp, "SpatialLines"),
byid = T)
# Create a small buffer around the VRA boundary line
vra_buffer <- gBuffer(
spgeom = vra_line,
width = 5e3)
# OLD: Find the 'border' counties using touching counties
# border_counties <- gTouches(
#   spgeom1 = us_shp,
#   spgeom2 = vra_line,
#   byid = T)
# Find the 'border' counties using intersection with buffered VRA border
border_counties <- gIntersects(
spgeom1 = us_shp,
spgeom2 = vra_buffer,
byid = T)
# Find the IDs of the border counties
border_ids <- names(border_counties[1,])[border_counties[1,] == T]
# Add a row ID to the US county shapefile
us_shp@data$ROWID <- rownames(us_shp@data)
# Subset of county shapefile for border counties
border_shp <- subset(us_shp, ROWID %in% border_ids)
# Find the VRA border counties
border_vra <- subset(border_shp,
(STATENAM %in% vra_all) |
((STATENAM == "North Carolina") & (NHGISNAM %in% nc_vra)))
# Find the non-VRA border counties
border_non <- subset(border_shp,
(!(STATENAM %in% vra_all) & (STATENAM != "North Carolina")) |
((STATENAM == "North Carolina") & (NHGISNAM %in% nc_non)))
plot(border_non)
plot(border_vra, col = "blue", add = T)
plot(vra_shp)
plot(non_shp, col = "blue", add = T)
plot(border_vra, col = "green", add = T)
# Convert objects to SLDF and SPDF ---------------------------------------------
#NOTE: We must convert the objects to be able to save them
# SLDF needs a data frame; create it using the length of the line
tmp_df <- data.frame(length = gLength(vra_line))
# Add ID to match the line's existing ID
row.names(tmp_df) <- vra_line@lines[[1]]@ID
# Create the SLDF
vra_sldf <- SpatialLinesDataFrame(sl = vra_line, data = tmp_df)
# Remove the data frame (no longer needed)
rm(tmp_df)
# Repeat the process for the aggregated shapefiles...
# For all_shp:
tmp_df <- data.frame(area = gArea(all_shp) / 1e6)
row.names(tmp_df) <- all_shp@polygons[[1]]@ID
all_spdf <- SpatialPolygonsDataFrame(Sr = all_shp, data = tmp_df)
rm(tmp_df)
# For non_shp:
tmp_df <- data.frame(area = gArea(non_shp) / 1e6)
row.names(tmp_df) <- non_shp@polygons[[1]]@ID
non_spdf <- SpatialPolygonsDataFrame(Sr = non_shp, data = tmp_df)
rm(tmp_df)
# For vra_shp:
tmp_df <- data.frame(area = gArea(vra_shp) / 1e6)
row.names(tmp_df) <- vra_shp@polygons[[1]]@ID
vra_spdf <- SpatialPolygonsDataFrame(Sr = vra_shp, data = tmp_df)
rm(tmp_df)
# Save shapefiles --------------------------------------------------------------
# Create the new directory path
dir_new <- paste0(dir_project, "DataSpatial/ShapefilesVRA/NorthCarolina")
# Create a folder for the shapefiles
dir.create(dir_new)
# Create subfolders
dir.create(paste0(dir_new, "/vra_line"))
dir.create(paste0(dir_new, "/vra_shp"))
dir.create(paste0(dir_new, "/non_shp"))
dir.create(paste0(dir_new, "/all_shp"))
dir.create(paste0(dir_new, "/border_shp"))
dir.create(paste0(dir_new, "/border_vra_shp"))
dir.create(paste0(dir_new, "/border_non_shp"))
# Save the VRA line
writeOGR(
obj = vra_sldf,
dsn = paste0(dir_new, "/vra_line"),
layer = "vra_line",
driver = "ESRI Shapefile")
# Save the shape files
writeOGR(
obj = all_spdf,
dsn = paste0(dir_new, "/all_shp"),
layer = "all_shp",
driver = "ESRI Shapefile",
overwrite_layer = T)
writeOGR(
obj = non_spdf,
dsn = paste0(dir_new, "/non_shp"),
layer = "non_shp",
driver = "ESRI Shapefile",
overwrite_layer = T)
writeOGR(
obj = vra_spdf,
dsn = paste0(dir_new, "/vra_shp"),
layer = "vra_shp",
driver = "ESRI Shapefile",
overwrite_layer = T)
writeOGR(
obj = border_shp,
dsn = paste0(dir_new, "/border_shp"),
layer = "border_shp",
driver = "ESRI Shapefile",
overwrite_layer = T)
writeOGR(
obj = border_vra,
dsn = paste0(dir_new, "/border_vra_shp"),
layer = "border_vra_shp",
driver = "ESRI Shapefile",
overwrite_layer = T)
writeOGR(
obj = border_non,
dsn = paste0(dir_new, "/border_non_shp"),
layer = "border_non_shp",
driver = "ESRI Shapefile",
overwrite_layer = T)
# Setup ----
# Options
options(stringsAsFactors = F)
options(scipen = 10)
# Packages
library(pacman)
p_load(readr, lfe, dplyr, magrittr, parallel, ggplot2, viridis, gmodels, msm)
# Directory
dir_section <- "/Users/edwardarubin/Dropbox/Teaching/ARE212/Section09/"
# My ggplot2 theme
theme_ed <- theme(
legend.position = "bottom",
panel.background = element_rect(fill = NA),
panel.border = element_rect(fill = NA, color = "grey75"),
axis.ticks = element_line(color = "grey85"),
panel.grid.major = element_line(color = "grey95", size = 0.2),
panel.grid.minor = element_line(color = "grey95", size = 0.2),
legend.key = element_blank())
# Load data ----
cars <- read_csv(paste0(dir_section, "auto.csv"))
# Functions ----
# Function to convert tibble, data.frame, or tbl_df to matrix
to_matrix <- function(the_df, vars) {
# Create a matrix from variables in var
new_mat <- the_df %>%
# Select the columns given in 'vars'
select_(.dots = vars) %>%
# Convert to matrix
as.matrix()
# Return 'new_mat'
return(new_mat)
}
# Function for OLS coefficient estimates
b_ols <- function(y, X) {
# Calculate beta hat
beta_hat <- solve(t(X) %*% X) %*% t(X) %*% y
# Return beta_hat
return(beta_hat)
}
# Function for OLS coef., SE, t-stat, and p-value
ols <- function(data, y_var, X_vars) {
# Turn data into matrices
y <- to_matrix(data, y_var)
X <- to_matrix(data, X_vars)
# Add intercept
X <- cbind(1, X)
# Calculate n and k for degrees of freedom
n <- nrow(X)
k <- ncol(X)
# Estimate coefficients
b <- b_ols(y, X)
# Update names
rownames(b)[1] <- "Intercept"
# Calculate OLS residuals
e <- y - X %*% b
# Calculate s^2
s2 <- (t(e) %*% e) / (n-k)
# Inverse of X'X
XX_inv <- solve(t(X) %*% X)
# Standard error
se <- sqrt(s2 * diag(XX_inv))
# Vector of _t_ statistics
t_stats <- (b - 0) / se
# Calculate the p-values
p_values = pt(q = abs(t_stats), df = n-k, lower.tail = F) * 2
# Nice table (data.frame) of results
results <- data.frame(
# The rows have the coef. names
effect = rownames(b),
# Estimated coefficients
coef = as.vector(b),
# Standard errors
std_error = as.vector(se),
# t statistics
t_stat = as.vector(t_stats),
# p-values
p_value = as.vector(p_values)
)
# Return the results
return(results)
}
ols(cars, "price", c("mpg", "weight"))
ols <- function(data, y_var, X_vars) {
# Turn data into matrices
y <- to_matrix(data, y_var)
X <- to_matrix(data, X_vars)
# Add intercept
X <- cbind(1, X)
# Calculate n and k for degrees of freedom
n <- nrow(X)
k <- ncol(X)
# Estimate coefficients
b <- b_ols(y, X)
# Update names
rownames(b)[1] <- "Intercept"
# Calculate OLS residuals
e <- y - X %*% b
# Calculate s^2
s2 <- (t(e) %*% e) / (n-k)
s2 %<>% as.numeric()
# Inverse of X'X
XX_inv <- solve(t(X) %*% X)
# Standard error
se <- sqrt(s2 * diag(XX_inv))
# Vector of _t_ statistics
t_stats <- (b - 0) / se
# Calculate the p-values
p_values = pt(q = abs(t_stats), df = n-k, lower.tail = F) * 2
# Nice table (data.frame) of results
results <- data.frame(
# The rows have the coef. names
effect = rownames(b),
# Estimated coefficients
coef = as.vector(b),
# Standard errors
std_error = as.vector(se),
# t statistics
t_stat = as.vector(t_stats),
# p-values
p_value = as.vector(p_values)
)
# Return the results
return(results)
}
ols(cars, "price", c("mpg", "weight"))
ols(cars, "price", c("mpg", "weight")) %>%
knitr::kable()
tmp_results <- ols(cars, "price", c("mpg", "weight"))[,2:5]
row.names(tmp_results) <- c("Intercept", "MPG", "Weight")
knitr::kable(tmp_results,
digits = c(2, 2, 2, 3),
col.names = c("$\\widehat{\\boldsymbol{\\beta}}$", "S.E.",
"___t___ stat", "___p___-Value"),
row.names = T,
caption = "Regressing price on mileage and weight"
)
vcov_ols <- function(data, y_var, X_vars) {
# Turn data into matrices
y <- to_matrix(data, y_var)
X <- to_matrix(data, X_vars)
# Add intercept
X <- cbind(1, X)
# Label intercept
colnames(X)[1] <- "intercept"
# Calculate n and k for degrees of freedom
n <- nrow(X)
k <- ncol(X)
# Estimate coefficients
b <- b_ols(y, X)
# Calculate residuals
e <- y - X %*% b
# Calculate s2 and convert to scalar
s2 <- t(e) %*% e / (n - k)
# Convert s2 to numeric
s2 %<>% as.numeric()
# Calculate the variance-covariance matrix
vcov_mat <- s2 * solve(t(X) %*% X)
# Return the variance-covariance matrix
return(vcov_mat)
}
vcov_ols(cars, "price", c("mpg", "weight"))
reg1 <- ols(cars, "price", c("mpg", "weight"))
(lc <- 20 * reg1[2,2] + 3000 * reg1[3,2])
# The variance-covariance matrix
vcov1 <- vcov_ols(cars, "price", c("mpg", "weight"))
(lc_se <- sqrt(20^2 * vcov1[2,2] + 3000^2 * vcov1[3,3] +
2 * 20 * 3000 * vcov1[2,3]))
lm_est <- lm(price ~ mpg + weight, data = cars)
estimable(obj = lm_est, cm = c(0, 20, 3000))
waldtest(lm_est, ~ 20 * mpg + 3000 * weight)
library(rmarkdown)
setwd("/Users/edwardarubin/Dropbox/Teaching/ARE212")
render("section09.Rmd",
pdf_document(
latex_engine = "xelatex",
toc = T,
number_sections = T,
highlight = "pygments",
pandoc_args = c(
"--metadata=author:\"Ed Rubin\"",
pandoc_variable_arg("mainfont", "Charter"),
pandoc_variable_arg("monofont", "Hack"),
pandoc_variable_arg("fontsize", "11pt")
)
),
output_dir = "Section09")
