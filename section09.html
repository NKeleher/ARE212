<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Section 9: Standard errors, Vol. I</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/flatly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/font-awesome-4.5.0/css/font-awesome.min.css" rel="stylesheet" />


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 60px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 65px;
  margin-top: -65px;
}

.section h2 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h3 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h4 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h5 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h6 {
  padding-top: 65px;
  margin-top: -65px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = false;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}

.tocify-subheader {
  display: inline;
}
.tocify-subheader .tocify-item {
  font-size: 0.95em;
}

</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-inverse  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">ARE 212</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Section notes
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="notes.html">Table of Contents</a>
    </li>
    <li>
      <a href="section00.html">Section 0</a>
    </li>
    <li>
      <a href="section01.html">Section 1</a>
    </li>
    <li>
      <a href="section02.html">Section 2</a>
    </li>
    <li>
      <a href="section03.html">Section 3</a>
    </li>
    <li>
      <a href="section04.html">Section 4</a>
    </li>
    <li>
      <a href="section05.html">Section 5</a>
    </li>
    <li>
      <a href="section06.html">Section 6</a>
    </li>
    <li>
      <a href="section07.html">Section 7</a>
    </li>
    <li>
      <a href="section08.html">Section 8</a>
    </li>
    <li>
      <a href="section09.html">Section 9</a>
    </li>
    <li>
      <a href="latexKnitr.html">LaTeX and knitr</a>
    </li>
  </ul>
</li>
<li>
  <a href="courseInfo.html">Course Info</a>
</li>
<li>
  <a href="syllabi.html">Syllabi</a>
</li>
<li>
  <a href="resources.html">R Resources</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="contact.html">
    <span class="fa fa-envelope-o fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="https://github.com/edrubin/ARE212">
    <span class="fa fa-github-square fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="https://edrub.in">
    <span class="fa fa-hand-peace-o fa-lg"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Section 9: Standard errors, Vol. I</h1>

</div>


<p><br></p>
<div id="admin" class="section level1">
<h1>Admin</h1>
<div id="last-week" class="section level2">
<h2>Last week</h2>
<p><a href="section08.html">Last week</a> we discussed the asymptotic properties of the OLS estimator under a more general set of assumptions.</p>
<p><strong>Follow-up</strong>: At the <a href="section08.html#are_we_in_asymptopia">end of last week’s section</a>, we discussed OLS’s asymptotic behavior with infinite-variance, Pareto-distributed disturbances. I suggested that the strange behavior was because we had not yet reached asymptopia. This suggestion was only sort of correct: with the infinite-variance specification of Pareto distribution, we will never reach asymptopia—no matter how large our sample size becomes. The reason: think back to the Lindeberg-Levy Central Limit Theorem… we need finite-variance disturbances to get</p>
<p><span class="math display">\[ \frac{1}{\sqrt{\mathstrut N}} \left( \sum_i^N \mathbf{x}_i^\prime \varepsilon_i \right) \overset{d}{\longrightarrow} \mathop{N}\left( 0, \boldsymbol{\Upsilon} \right) \]</span></p>
<p>Put simply: no convergence without finite-variance.</p>
</div>
<div id="current-problem-sets" class="section level2">
<h2>Current problem sets</h2>
<p>Due this Friday and next Wednesday. Again, please try to submit in a way where your answers are outside of and clearly distinguishable from your R code. And please include the R code for each problem along with the problem (unless it draws upon something you already calculated—no need to copy the same code 100 times).</p>
</div>
<div id="this-week" class="section level2">
<h2>This week</h2>
<p>Standard errors. Specifically standard errors of linear and nonlinear combinations of OLS-estimated parameters, which will bring us to the Delta Method. Finally: making (pretty) tables.</p>
</div>
<div id="what-you-will-need" class="section level2">
<h2>What you will need</h2>
<p><strong>Packages</strong>:</p>
<ul>
<li>New! (You probably need to install these packages):
<ul>
<li><code>gmodels</code> for its function <code>estimable()</code></li>
<li><code>msm</code> for its Delta Method function <code>deltamethod()</code></li>
</ul></li>
<li>Previously used: <code>dplyr</code>, <code>readr</code>, <code>magrittr</code>, <code>lfe</code>, <code>ggplot2</code>, <code>ggthemes</code></li>
</ul>
<p><strong>Data</strong>: The <a href="Section09/auto.csv"><code>auto.csv</code></a> file (again).</p>
</div>
</div>
<div id="standard-errors" class="section level1">
<h1>Standard errors</h1>
<p>As we discussed <a href="section05.html">previously</a>, inference is tremendously important in econometrics. And at the heart of inference is the issue of calculating standard errors. Why? If we want to test the significance of our estimates—or if we want to construct a confidence interval for our estimates or predictions—then we need to calculate (estimate) the variance around our point estimates. And what is the standard deviation of an estimator? Its standard error.</p>
<div id="setup" class="section level2">
<h2>Setup</h2>
<p>Let’s set up R and load our functions and data.</p>
<div id="setup-and-data" class="section level3">
<h3>Setup and data</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Setup ----</span>
<span class="co"># Options</span>
<span class="kw">options</span>(<span class="dt">stringsAsFactors =</span> F)
<span class="kw">options</span>(<span class="dt">scipen =</span> <span class="dv">10</span>)
<span class="co"># Packages</span>
<span class="kw">library</span>(readr)
<span class="kw">library</span>(lfe)
<span class="kw">library</span>(dplyr)
<span class="kw">library</span>(magrittr)
<span class="kw">library</span>(parallel)
<span class="kw">library</span>(ggplot2)
<span class="kw">library</span>(viridis)
<span class="kw">library</span>(gmodels)
<span class="kw">library</span>(msm)
<span class="co"># Directory</span>
<span class="kw">setwd</span>(<span class="st">&quot;/Users/edwardarubin/Dropbox/Teaching/ARE212/Section09&quot;</span>)
<span class="co"># My ggplot2 theme</span>
theme_ed &lt;-<span class="st"> </span><span class="kw">theme</span>(
  <span class="dt">legend.position =</span> <span class="st">&quot;bottom&quot;</span>,
  <span class="dt">panel.background =</span> <span class="kw">element_rect</span>(<span class="dt">fill =</span> <span class="ot">NA</span>),
  <span class="dt">panel.border =</span> <span class="kw">element_rect</span>(<span class="dt">fill =</span> <span class="ot">NA</span>, <span class="dt">color =</span> <span class="st">&quot;grey75&quot;</span>),
  <span class="dt">axis.ticks =</span> <span class="kw">element_line</span>(<span class="dt">color =</span> <span class="st">&quot;grey85&quot;</span>),
  <span class="dt">panel.grid.major =</span> <span class="kw">element_line</span>(<span class="dt">color =</span> <span class="st">&quot;grey95&quot;</span>, <span class="dt">size =</span> <span class="fl">0.2</span>),
  <span class="dt">panel.grid.minor =</span> <span class="kw">element_line</span>(<span class="dt">color =</span> <span class="st">&quot;grey95&quot;</span>, <span class="dt">size =</span> <span class="fl">0.2</span>),
  <span class="dt">legend.key =</span> <span class="kw">element_blank</span>())

<span class="co"># Load data ----</span>
cars &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;auto.csv&quot;</span>)</code></pre></div>
</div>
<div id="functions" class="section level3">
<h3>Functions</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Functions ----</span>
<span class="co"># Function to convert tibble, data.frame, or tbl_df to matrix</span>
to_matrix &lt;-<span class="st"> </span>function(the_df, vars) {
  <span class="co"># Create a matrix from variables in var</span>
  new_mat &lt;-<span class="st"> </span>the_df %&gt;%
<span class="st">    </span><span class="co"># Select the columns given in &#39;vars&#39;</span>
<span class="st">    </span><span class="kw">select_</span>(<span class="dt">.dots =</span> vars) %&gt;%
<span class="st">    </span><span class="co"># Convert to matrix</span>
<span class="st">    </span><span class="kw">as.matrix</span>()
  <span class="co"># Return &#39;new_mat&#39;</span>
  <span class="kw">return</span>(new_mat)
}
<span class="co"># Function for OLS coefficient estimates</span>
b_ols &lt;-<span class="st"> </span>function(y, X) {
  <span class="co"># Calculate beta hat</span>
  beta_hat &lt;-<span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(X) %*%<span class="st"> </span>X) %*%<span class="st"> </span><span class="kw">t</span>(X) %*%<span class="st"> </span>y
  <span class="co"># Return beta_hat</span>
  <span class="kw">return</span>(beta_hat)
}
<span class="co"># Function for OLS coef., SE, t-stat, and p-value</span>
ols &lt;-<span class="st"> </span>function(data, y_var, X_vars) {
  <span class="co"># Turn data into matrices</span>
  y &lt;-<span class="st"> </span><span class="kw">to_matrix</span>(data, y_var)
  X &lt;-<span class="st"> </span><span class="kw">to_matrix</span>(data, X_vars)
  <span class="co"># Add intercept</span>
  X &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="dv">1</span>, X)
  <span class="co"># Calculate n and k for degrees of freedom</span>
  n &lt;-<span class="st"> </span><span class="kw">nrow</span>(X)
  k &lt;-<span class="st"> </span><span class="kw">ncol</span>(X)
  <span class="co"># Estimate coefficients</span>
  b &lt;-<span class="st"> </span><span class="kw">b_ols</span>(y, X)
  <span class="co"># Update names</span>
  <span class="kw">rownames</span>(b)[<span class="dv">1</span>] &lt;-<span class="st"> &quot;Intercept&quot;</span>
  <span class="co"># Calculate OLS residuals</span>
  e &lt;-<span class="st"> </span>y -<span class="st"> </span>X %*%<span class="st"> </span>b
  <span class="co"># Calculate s^2</span>
  s2 &lt;-<span class="st"> </span>(<span class="kw">t</span>(e) %*%<span class="st"> </span>e) /<span class="st"> </span>(n-k)
  <span class="co"># Inverse of X&#39;X</span>
  XX_inv &lt;-<span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(X) %*%<span class="st"> </span>X)
  <span class="co"># Standard error</span>
  se &lt;-<span class="st"> </span><span class="kw">sqrt</span>(s2 *<span class="st"> </span><span class="kw">diag</span>(XX_inv))
  <span class="co"># Vector of _t_ statistics</span>
  t_stats &lt;-<span class="st"> </span>(b -<span class="st"> </span><span class="dv">0</span>) /<span class="st"> </span>se
  <span class="co"># Calculate the p-values</span>
  p_values =<span class="st"> </span><span class="kw">pt</span>(<span class="dt">q =</span> <span class="kw">abs</span>(t_stats), <span class="dt">df =</span> n-k, <span class="dt">lower.tail =</span> F) *<span class="st"> </span><span class="dv">2</span>
  <span class="co"># Nice table (data.frame) of results</span>
  results &lt;-<span class="st"> </span><span class="kw">data.frame</span>(
    <span class="co"># The rows have the coef. names</span>
    <span class="dt">effect =</span> <span class="kw">rownames</span>(b),
    <span class="co"># Estimated coefficients</span>
    <span class="dt">coef =</span> <span class="kw">as.vector</span>(b),
    <span class="co"># Standard errors</span>
    <span class="dt">std_error =</span> <span class="kw">as.vector</span>(se),
    <span class="co"># t statistics</span>
    <span class="dt">t_stat =</span> <span class="kw">as.vector</span>(t_stats),
    <span class="co"># p-values</span>
    <span class="dt">p_value =</span> <span class="kw">as.vector</span>(p_values)
    )
  <span class="co"># Return the results</span>
  <span class="kw">return</span>(results)
}</code></pre></div>
</div>
</div>
<div id="tables" class="section level2">
<h2>Tables</h2>
<p>Suppose we want to estimate the following model.</p>
<p><span class="math display">\[ \text{Price}_i = \beta_0 + \beta_1 \text{MPG}_i + \beta_2 \text{Weight}_i + \varepsilon_i \]</span></p>
<p>We know how to calculate the coefficients, the standard errors, the <em>t</em> statistics, and the <em>p</em>-values, <em>e.g.</em>,</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Regress price on MPG and weight</span>
<span class="kw">ols</span>(cars, <span class="st">&quot;price&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>, <span class="st">&quot;weight&quot;</span>))</code></pre></div>
<pre><code>##      effect        coef    std_error     t_stat     p_value
## 1 Intercept 1946.068668 3597.0495988  0.5410180 0.590188628
## 2       mpg  -49.512221   86.1560389 -0.5746808 0.567323727
## 3    weight    1.746559    0.6413538  2.7232382 0.008129813</code></pre>
<p>Can we make the results a bit prettier? Let’s grab the results and feed them to the <code>kable()</code> function from the <code>knitr</code> package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Regress price on MPG and weight</span>
<span class="kw">ols</span>(cars, <span class="st">&quot;price&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>, <span class="st">&quot;weight&quot;</span>)) %&gt;%
<span class="st">  </span>knitr::<span class="kw">kable</span>()</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left">effect</th>
<th align="right">coef</th>
<th align="right">std_error</th>
<th align="right">t_stat</th>
<th align="right">p_value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="right">1946.068668</td>
<td align="right">3597.0495988</td>
<td align="right">0.5410180</td>
<td align="right">0.5901886</td>
</tr>
<tr class="even">
<td align="left">mpg</td>
<td align="right">-49.512221</td>
<td align="right">86.1560389</td>
<td align="right">-0.5746808</td>
<td align="right">0.5673237</td>
</tr>
<tr class="odd">
<td align="left">weight</td>
<td align="right">1.746559</td>
<td align="right">0.6413538</td>
<td align="right">2.7232382</td>
<td align="right">0.0081298</td>
</tr>
</tbody>
</table>
<p>Not bad, but we can do more.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Regress price on MPG and weight</span>
tmp_results &lt;-<span class="st"> </span><span class="kw">ols</span>(cars, <span class="st">&quot;price&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>, <span class="st">&quot;weight&quot;</span>))[,<span class="dv">2</span>:<span class="dv">5</span>]
<span class="kw">row.names</span>(tmp_results) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Intercept&quot;</span>, <span class="st">&quot;MPG&quot;</span>, <span class="st">&quot;Weight&quot;</span>)
knitr::<span class="kw">kable</span>(tmp_results,
  <span class="dt">digits =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>),
  <span class="dt">col.names =</span> <span class="kw">c</span>(<span class="st">&quot;$</span><span class="ch">\\</span><span class="st">widehat{</span><span class="ch">\\</span><span class="st">boldsymbol{</span><span class="ch">\\</span><span class="st">beta}}$&quot;</span>, <span class="st">&quot;S.E.&quot;</span>,
    <span class="st">&quot;___t___ stat&quot;</span>, <span class="st">&quot;___p___-Value&quot;</span>),
  <span class="dt">row.names =</span> T,
  <span class="dt">caption =</span> <span class="st">&quot;Regressing price on mileage and weight&quot;</span>
  )</code></pre></div>
<table>
<caption>Regressing price on mileage and weight</caption>
<thead>
<tr class="header">
<th></th>
<th align="right"><span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span></th>
<th align="right">S.E.</th>
<th align="right"><strong><em>t</em></strong> stat</th>
<th align="right"><strong><em>p</em></strong>-Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Intercept</td>
<td align="right">1946.07</td>
<td align="right">3597.05</td>
<td align="right">0.54</td>
<td align="right">0.590</td>
</tr>
<tr class="even">
<td>MPG</td>
<td align="right">-49.51</td>
<td align="right">86.16</td>
<td align="right">-0.57</td>
<td align="right">0.567</td>
</tr>
<tr class="odd">
<td>Weight</td>
<td align="right">1.75</td>
<td align="right">0.64</td>
<td align="right">2.72</td>
<td align="right">0.008</td>
</tr>
</tbody>
</table>
<p>Because I write the section notes in Rmarkdown, I am making use of Markdown formatting within the column names (<em>e.g.</em>, <code>___t___</code> creates a bolded, italicized <code>t</code>, <em>i.e.</em>, <strong><em>t</em></strong>). If you want LaTeX formatting, then you can set the <code>format</code> option to <code>&quot;latex&quot;</code>, to generate the LaTeX code for a table. You will also want to set <code>escape = F</code> if you want <code>knitr</code> to print the table as a table with LaTeX math expressions inside. I’m also using the <code>booktabs = T</code> argument, which creates prettier tables in LaTeX but also requires adding <code>\usepackage{booktabs}</code> to your preamble.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Regress price on MPG and weight</span>
tmp_results &lt;-<span class="st"> </span><span class="kw">ols</span>(cars, <span class="st">&quot;price&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>, <span class="st">&quot;weight&quot;</span>))[,<span class="dv">2</span>:<span class="dv">5</span>]
<span class="kw">row.names</span>(tmp_results) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Intercept&quot;</span>, <span class="st">&quot;MPG&quot;</span>, <span class="st">&quot;Weight&quot;</span>)
knitr::<span class="kw">kable</span>(tmp_results,
  <span class="dt">format =</span> <span class="st">&quot;latex&quot;</span>,
  <span class="dt">digits =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>),
  <span class="dt">col.names =</span> <span class="kw">c</span>(<span class="st">&quot;$</span><span class="ch">\\</span><span class="st">widehat{</span><span class="ch">\\</span><span class="st">boldsymbol{</span><span class="ch">\\</span><span class="st">beta}}$&quot;</span>, <span class="st">&quot;S.E.&quot;</span>,
    <span class="st">&quot;$t$ stat&quot;</span>, <span class="st">&quot;$p$-Value&quot;</span>),
  <span class="dt">escape =</span> F,
  <span class="dt">row.names =</span> T,
  <span class="dt">caption =</span> <span class="st">&quot;Regressing price on mileage and weight&quot;</span>,
  <span class="dt">booktabs =</span> T
  ) %&gt;%<span class="st"> </span><span class="kw">print</span>()</code></pre></div>
<pre><code>## \begin{table}
## 
## \caption{\label{tab:unnamed-chunk-6}Regressing price on mileage and weight}
## \centering
## \begin{tabular}[t]{lrrrr}
## \toprule
##   &amp; $\widehat{\boldsymbol{\beta}}$ &amp; S.E. &amp; $t$ stat &amp; $p$-Value\\
## \midrule
## Intercept &amp; 1946.07 &amp; 3597.05 &amp; 0.54 &amp; 0.590\\
## MPG &amp; -49.51 &amp; 86.16 &amp; -0.57 &amp; 0.567\\
## Weight &amp; 1.75 &amp; 0.64 &amp; 2.72 &amp; 0.008\\
## \bottomrule
## \end{tabular}
## \end{table}</code></pre>
<p>I’m using the <code>print()</code> function above to tell my Rmarkdown compiler to print to code for the table—as opposed to attempting to create the table (so you can see the code). You don’t need the <code>print()</code>.</p>
<p>This is about as far as <code>knitr</code> and <code>kable()</code> can take us in table creation. For more features, I suggest the packages <code>stargazer</code> and/or <code>xtable</code>. (For a future section.)</p>
</div>
<div id="linear-combinations" class="section level2">
<h2>Linear combinations</h2>
<p>Okay, so we know how to calculate the standard error for out point estimates of the coefficients. What if we would like to know the average “effect” on price for 20 MPG and 3,000 pounds?<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> Let us call this “mean effect” <span class="math inline">\(lc\)</span> (for linear combination). Thus, we are interested in <span class="math inline">\(lc = 20\beta_1 + 3000\beta_2\)</span>. We will estimate <span class="math inline">\(lc\)</span> via</p>
<p><span class="math display">\[ \widehat{LC} = 20\times b_1 + 3000\times b_2 \]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Regress price on mpg and weight</span>
reg1 &lt;-<span class="st"> </span><span class="kw">ols</span>(cars, <span class="st">&quot;price&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>, <span class="st">&quot;weight&quot;</span>))
<span class="co"># lc = 20 * b1 + 3000 * b2</span>
(lc &lt;-<span class="st"> </span><span class="dv">20</span> *<span class="st"> </span>reg1[<span class="dv">2</span>,<span class="dv">2</span>] +<span class="st"> </span><span class="dv">3000</span> *<span class="st"> </span>reg1[<span class="dv">3</span>,<span class="dv">2</span>])</code></pre></div>
<pre><code>## [1] 4249.433</code></pre>
<p><em>Aside</em>: Wrapping the definition of an object in parentheses forces R to print the object’s value.</p>
<p>We have a point estimate. We’re done, right? Noooooo! Point estimates without standard errors should make you a bit suspicious. How precisely estimated is the point estimate? Is there any evidence it is significantly different from zero?</p>
<p>So how can we get a standard error for this point estimate? There are two common routes—an analytical route and the Delta-Method route.</p>
</div>
<div id="route-1-analytical-variance" class="section level2">
<h2>Route 1: Analytical variance</h2>
<p>Let’s revisit the definition of a standard error. When defined the standard error of <span class="math inline">\(\mathbf{b}\)</span>, as</p>
<p><span class="math display">\[\text{se}(\mathbf{b}) = \sqrt{\text{Var}\left( \mathbf{b} \right)}\]</span></p>
<p>More generally, the standard error of an arbitrary estimator <span class="math inline">\(\theta\)</span> is simply</p>
<p><span class="math display">\[\text{se}(\mathbf{\theta}) = \sqrt{\text{Var}\left( \mathbf{\theta} \right)}\]</span></p>
<p>Thus, if we want the standard error of <span class="math inline">\(\widehat{LC}\)</span>, we really need know the variance of <span class="math inline">\(\widehat{LC}\)</span>. So what is the variance of <span class="math inline">\(\widehat{LC}\)</span>?</p>
<p><span class="math display">\[ \mathop{\text{Var}} \left( \widehat{LC} \right) = \mathop{\text{Var}} \left( 20 b_1 + 3000 b_2 \right) \]</span></p>
<p>Now let’s think way back to elementary statistics. There are a few relationships that will be useful here:</p>
<p><span class="math display">\[ \mathop{\text{Var}} \left( aX \right) = a^2 \mathop{\text{Var}} \left( X \right) \]</span></p>
<p><span class="math display">\[ \mathop{\text{Var}} \left( X + Y \right) = \mathop{\text{Var}} \left( X \right) + \mathop{\text{Var}} \left( Y \right) + \mathop{\text{Cov}} \left(X,Y\right) \]</span></p>
<p><span class="math display">\[ \mathop{\text{Cov}} \left( aX,bY \right) = ab \mathop{\text{Cov}} \left( X,Y \right)\]</span></p>
<p>which leaves us with</p>
<p><span class="math display">\[ \mathop{\text{Var}} \left( aX + bY \right) = a^2 \mathop{\text{Var}} \left( X \right) + b^2 \mathop{\text{Var}} \left( Y \right) + 2ab \mathop{\text{Cov}} \left(X,Y\right) \]</span></p>
<p>Now we can apply this knowledge to <span class="math inline">\(\mathop{\text{Var}} \left( \widehat{LC} \right)\)</span>:</p>
<p><span class="math display">\[ \mathop{\text{Var}} \left( \widehat{LC} \right) =
20^2 \mathop{\text{Var}} \left( b_1 \right) + 3000^2 \mathop{\text{Var}} \left( b_2 \right) + 2 \times 20 \times 3000 \mathop{\text{Cov}} \left( b_1, b_2 \right)\]</span></p>
<p>Lucky for us, we’ve already calculated estimates for <span class="math inline">\(\mathop{\text{Var}} \left( b_1 \right)\)</span>, <span class="math inline">\(\mathop{\text{Var}} \left( b_2 \right)\)</span>, and <span class="math inline">\(\mathop{\text{Cov}} \left( b_1, b_2 \right)\)</span>: they are (some of) the elements of <span class="math inline">\(s^2 \left(\mathbf{X}^\prime \mathbf{X}\right)^{-1}\)</span>. (Specifically, they are the elements of the variance-covariance matrix once we remove the first row and column.)</p>
<p>Let’s write a quick function that returns the variance-covariance matrix of <span class="math inline">\(\mathbf{b}\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Variance-covariance function for OLS beta hat</span>
vcov_ols &lt;-<span class="st"> </span>function(data, y_var, X_vars) {
  <span class="co"># Turn data into matrices</span>
  y &lt;-<span class="st"> </span><span class="kw">to_matrix</span>(data, y_var)
  X &lt;-<span class="st"> </span><span class="kw">to_matrix</span>(data, X_vars)
  <span class="co"># Add intercept</span>
  X &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="dv">1</span>, X)
  <span class="co"># Label intercept</span>
  <span class="kw">colnames</span>(X)[<span class="dv">1</span>] &lt;-<span class="st"> &quot;intercept&quot;</span>
  <span class="co"># Calculate n and k for degrees of freedom</span>
  n &lt;-<span class="st"> </span><span class="kw">nrow</span>(X)
  k &lt;-<span class="st"> </span><span class="kw">ncol</span>(X)
  <span class="co"># Estimate coefficients</span>
  b &lt;-<span class="st"> </span><span class="kw">b_ols</span>(y, X)
  <span class="co"># Calculate residuals</span>
  e &lt;-<span class="st"> </span>y -<span class="st"> </span>X %*%<span class="st"> </span>b
  <span class="co"># Calculate s2 and convert to scalar</span>
  s2 &lt;-<span class="st"> </span>(<span class="kw">t</span>(e) %*%<span class="st"> </span>e /<span class="st"> </span>(n -<span class="st"> </span>k)) %&gt;%<span class="st"> </span><span class="kw">as.vector</span>()
  <span class="co"># Calculate the variance-covariance matrix</span>
  vcov_mat &lt;-<span class="st"> </span>s2 *<span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(X) %*%<span class="st"> </span>X)
  <span class="co"># Return the variance-covariance matrix</span>
  <span class="kw">return</span>(vcov_mat)
}</code></pre></div>
<p>First, let’s make sure our function works.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Run the vcov_ols() function</span>
<span class="kw">vcov_ols</span>(cars, <span class="st">&quot;price&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>, <span class="st">&quot;weight&quot;</span>))</code></pre></div>
<pre><code>##              intercept           mpg        weight
## intercept 12938765.816 -292759.82264 -2191.9031965
## mpg        -292759.823    7422.86303    44.6016592
## weight       -2191.903      44.60166     0.4113347</code></pre>
<p>This matrix contains our estimates for</p>
<p><span class="math display">\[\mathop{\text{Var}} \left( \mathbf{b} \right) =
\left[\begin{array}{ccc}
\mathop{\text{Var}} \left( b_0 \right) &amp; \mathop{\text{Cov}} \left( b_0, b_1 \right) &amp; \mathop{\text{Cov}} \left( b_0, b_2 \right) \\
\mathop{\text{Cov}} \left( b_0, b_1 \right) &amp; \mathop{\text{Var}} \left( b_1 \right) &amp; \mathop{\text{Cov}} \left( b_0, b_2 \right) \\
\mathop{\text{Cov}} \left( b_0, b_2 \right) &amp; \mathop{\text{Cov}} \left( b_1, b_2 \right) &amp; \mathop{\text{Var}} \left( b_2 \right)
\end{array}\right]
\]</span></p>
<p>Looks great. Now let’s calculate the (analytical) standard error for <span class="math inline">\(\widehat{LC}\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Regress price on mpg and weight</span>
reg1 &lt;-<span class="st"> </span><span class="kw">ols</span>(cars, <span class="st">&quot;price&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>, <span class="st">&quot;weight&quot;</span>))
<span class="co"># lc = 20 * b1 + 3000 * b2</span>
(lc &lt;-<span class="st"> </span><span class="dv">20</span> *<span class="st"> </span>reg1[<span class="dv">2</span>,<span class="dv">2</span>] +<span class="st"> </span><span class="dv">3000</span> *<span class="st"> </span>reg1[<span class="dv">3</span>,<span class="dv">2</span>])</code></pre></div>
<pre><code>## [1] 4249.433</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># The variance-covariance matrix</span>
vcov1 &lt;-<span class="st"> </span><span class="kw">vcov_ols</span>(cars, <span class="st">&quot;price&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>, <span class="st">&quot;weight&quot;</span>))
<span class="co"># The standard error for &#39;lc&#39;</span>
(lc_se &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">20</span>^<span class="dv">2</span> *<span class="st"> </span>vcov1[<span class="dv">2</span>,<span class="dv">2</span>] +<span class="st"> </span><span class="dv">3000</span>^<span class="dv">2</span> *<span class="st"> </span>vcov1[<span class="dv">3</span>,<span class="dv">3</span>] +
<span class="st">  </span><span class="dv">2</span> *<span class="st"> </span><span class="dv">20</span> *<span class="st"> </span><span class="dv">3000</span> *<span class="st"> </span>vcov1[<span class="dv">2</span>,<span class="dv">3</span>]))</code></pre></div>
<pre><code>## [1] 3467.471</code></pre>
<p>Let’s check our work using the canned <code>lm()</code> function in conjunction with the <code>estimable()</code> function<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> from the <code>gmodels</code> package that we previously loaded.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> The <code>estimable()</code> function estimates the point estimate and the standard error for a linear combination of coefficients from an estimated model object <code>obj</code>. We pass <code>estimable()</code> the linear combination via its argument <code>cm</code>. In our case, <code>cm = c(0, 20, 3000)</code>, meaning we do not want the intercept, we want to multiply <span class="math inline">\(b_1\)</span> coefficient by 20, and we want to multiply the third coefficient <span class="math inline">\(b_2\)</span> by 3,000.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Estimate the model with &#39;lm&#39;</span>
lm_est &lt;-<span class="st"> </span><span class="kw">lm</span>(price ~<span class="st"> </span>mpg +<span class="st"> </span>weight, <span class="dt">data =</span> cars)
<span class="co"># Estimate the linear combination</span>
<span class="kw">estimable</span>(<span class="dt">obj =</span> lm_est, <span class="dt">cm =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">20</span>, <span class="dv">3000</span>))</code></pre></div>
<pre><code>##             Estimate Std. Error  t value DF  Pr(&gt;|t|)
## (0 20 3000) 4249.433   3467.471 1.225514 71 0.2244315</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Alternative test (no standard errors, though)</span>
<span class="kw">waldtest</span>(lm_est, ~<span class="st"> </span><span class="dv">20</span> *<span class="st"> </span>mpg +<span class="st"> </span><span class="dv">3000</span> *<span class="st"> </span>weight)</code></pre></div>
<pre><code>##          p       chi2        df1        p.F          F        df2 
##  0.2203818  1.5018836  1.0000000  0.2244315  1.5018836 71.0000000 
## attr(,&quot;formula&quot;)
## ~20 * mpg + 3000 * weight
## &lt;environment: 0x7f849fac2a30&gt;</code></pre>
<p>I’ve also added <code>lfe</code>’s <code>waldtest()</code> function. It gives a point estimate and tests against zero but does not provide a standard error.</p>
<p>How could we build our own <em>t</em> statistic?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Our t statistic</span>
lc /<span class="st"> </span>lc_se</code></pre></div>
<pre><code>## [1] 1.225514</code></pre>
<p><em>Quick summary</em>: our point estimate, while seemingly large, is not significantly different from zero.</p>
</div>
<div id="route-2-delta-method" class="section level2">
<h2>Route 2: Delta Method</h2>
<p>There are times where you either cannot (or do not want to) solve analytically for the variance of your estimator. Enter: the Delta Method.</p>
<p>The Delta Method can look a bit intimidating, but if you stick with me through the math, you will find it actually is not too bad.</p>
<p>Take an arbitrary function <span class="math inline">\(\mathbf{a(\cdot)}:\mathbb{R}^K\rightarrow \mathbb{R}^r\)</span> (whose first derivatives exist and are continuous). (This <span class="math inline">\(\mathbf{a}(\cdot)\)</span> is generally going to be some sort of function of our coefficients.)</p>
<p>Now define <span class="math inline">\(\mathbf{A}(\boldsymbol{\beta})\)</span> as the <span class="math inline">\(r\times k\)</span> matrix of first derivatives, evaluated at <span class="math inline">\(\boldsymbol{\beta}\)</span>,</p>
<p><span class="math display">\[ \mathop{\mathbf{A}}\left(\boldsymbol{\beta}\right) = \dfrac{\partial \mathop{\mathbf{a}}\left(\boldsymbol{\beta}\right)}{\partial \boldsymbol{\beta}^\prime} \]</span></p>
<p>Now take a sequence of <span class="math inline">\(k\)</span>-dimensional random vectors <span class="math inline">\(\{\mathbf{x}_N:\: N=1,\,2,\,\ldots\}\)</span>, where</p>
<p><span class="math display">\[ \mathbf{x}_N\overset{p}{\rightarrow}\boldsymbol{\beta} \]</span></p>
<p>and</p>
<p><span class="math display">\[ \sqrt{\mathstrut N}\left(\mathbf{x}_N  - \boldsymbol{\beta}\right) \overset{d}{\longrightarrow} \mathop{N}\left(\boldsymbol{0}, \boldsymbol{\Sigma}\right) \]</span></p>
<p>This “sequence” will generally be our (OLS) estimator (it converges to <span class="math inline">\(\boldsymbol{\beta}\)</span> and is asymptotically normal).</p>
<p>If we satisfy these conditions, then</p>
<p><span class="math display">\[ \sqrt{\mathstrut N} \big( \mathop{\mathbf{a}}\left( \mathbf{x}_N \right) - \mathop{\mathbf{a}}(\boldsymbol{\beta}) \big) \overset{d}{\longrightarrow} N \big( \mathbf{0}, \mathop{\mathbf{A}}(\boldsymbol{\beta})\boldsymbol{\Sigma}\mathop{\mathbf{A}}(\boldsymbol{\beta})^\prime \big) \]</span></p>
<p>So what does all of this math (the Delta Method) mean?</p>
<p>Imagine we want to estimate some function of unknown parameters <span class="math inline">\(\mathop{\mathbf{a}}(\boldsymbol{\beta})\)</span>. The Delta Method tells us that if we have some estimator (sequence) <span class="math inline">\(\mathbf{x}_N\)</span> that is consistent for <span class="math inline">\(\boldsymbol{\beta}\)</span> and asymptotically normal—and if the first derivatives of <span class="math inline">\(\mathop{\mathbf{a}}(\cdot)\)</span> exist and are continuous at <span class="math inline">\(\boldsymbol{\beta}\)</span>—then</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\mathop{\mathbf{a}}\left( \mathbf{x}_N \right)\)</span> is consistent for <span class="math inline">\(\mathop{\mathbf{a}}\left( \boldsymbol{\beta} \right)\)</span>, <em>i.e.</em>, we can plug our estimates for <span class="math inline">\(\boldsymbol{\beta}\)</span> into <span class="math inline">\(\mathbf{a}(\cdot)\)</span> to estimate <span class="math inline">\(\mathop{\mathbf{a}}\left( \mathbf{x}_N \right)\)</span></li>
<li>The variance-covariance matrix of this new estimator <span class="math inline">\(\mathbf{a}\left( \mathbf{x}_N \right)\)</span> is <span class="math inline">\(\mathop{\mathbf{A}}(\boldsymbol{\beta})\boldsymbol{\Sigma}\mathop{\mathbf{A}}(\boldsymbol{\beta})^\prime\)</span>. In practice, we need to take derivatives of <span class="math inline">\(\mathbf{a}(\cdot)\)</span> with respect to <span class="math inline">\(\boldsymbol{\beta}\)</span> and then plug in estimates.</li>
</ol>
<p>Let’s see what the Delta Method looks like in an actual application.</p>
<p>Recall our function of unknown parameters</p>
<p><span class="math display">\[ \mathop{\mathbf{a}}(\boldsymbol{\beta}) = LC = 20\beta_1 + 3000\beta_2 \]</span></p>
<p>and its estimator</p>
<p><span class="math display">\[ \mathop{\mathbf{a}}(\mathbf{x}_N) = \mathop{\mathbf{a}}(\mathbf{b}_\text{OLS}) =  \widehat{LC} = 20 \times b_1 + 3000 \times b_2 \]</span></p>
<p>Do we satisfy the requirements of the Delta Method?</p>
<ol style="list-style-type: decimal">
<li>We know <span class="math inline">\(\mathbf{b}_\text{OLS}\)</span> is consistent for <span class="math inline">\(\boldsymbol{\beta}\)</span></li>
<li>We also know <span class="math inline">\(\sqrt{\mathstrut N}\left(\mathbf{b}_\text{OLS} - \boldsymbol{\beta}\right) \overset{d}{\longrightarrow} \mathop{N}\left(\boldsymbol{0}, \sigma^2 \left(\mathbf{X}^\prime \mathbf{X}\right)^{-1} \right)\)</span></li>
<li>The continuous-derivatives condition is satisfied, as</li>
</ol>
<p><span class="math display">\[ \mathop{\mathbf{A}}\left(\boldsymbol{\beta}\right) = \dfrac{\partial \mathop{\mathbf{a}}\left(\boldsymbol{\beta}\right)}{\partial \boldsymbol{\beta}^\prime} = \dfrac{\partial (20\beta_1 + 3000\beta_2)}{\partial [\beta_0,\, \beta_1,\, \beta_2]} =
\left[\begin{array}{ccc}
0 &amp; 20 &amp; 3000
\end{array}
\right]\]</span></p>
<p>Let’s name this derivative matrix <span class="math inline">\(\mathbf{LC}_\beta\)</span>.</p>
<p>Because we satisfy these conditions, we can apply the Delta Method, <em>i.e.</em>,</p>
<p><span class="math display">\[ \sqrt{\mathstrut N} \big(\widehat{LC} - LC\big) \overset{d}{\longrightarrow} \mathop{N}\big(0, \mathbf{LC}_\beta \cdot \sigma^2 \left(\mathbf{X}^\prime \mathbf{X}\right)^{-1} \cdot \mathbf{LC}_\beta^\prime \big) \]</span></p>
<p>Alright! We made it. Now let’s put this last statement in code (estimating <span class="math inline">\(\sigma^2\)</span> with <span class="math inline">\(s^2\)</span>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Remind ourselves of LC and its var-cov matrix</span>
lc &lt;-<span class="st"> </span><span class="dv">20</span> *<span class="st"> </span>reg1[<span class="dv">2</span>,<span class="dv">2</span>] +<span class="st"> </span><span class="dv">3000</span> *<span class="st"> </span>reg1[<span class="dv">3</span>,<span class="dv">2</span>]
vcov1 &lt;-<span class="st"> </span><span class="kw">vcov_ols</span>(cars, <span class="st">&quot;price&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>, <span class="st">&quot;weight&quot;</span>))
<span class="co"># Define our derivative matrix</span>
deriv_mat &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">20</span>, <span class="dv">3000</span>), <span class="dt">nrow =</span> <span class="dv">1</span>)
<span class="co"># Calculate the standard error of &#39;lc&#39; via delta method</span>
lc_dm &lt;-<span class="st"> </span><span class="kw">sqrt</span>(deriv_mat %*%<span class="st"> </span>vcov1 %*%<span class="st"> </span><span class="kw">t</span>(deriv_mat))</code></pre></div>
<p>Finally, let’s compare the two sets of standard errors—analytical and Delta Method</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Analytical s.e.</span>
lc_se</code></pre></div>
<pre><code>## [1] 3467.471</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Delta Method s.e.</span>
lc_dm</code></pre></div>
<pre><code>##          [,1]
## [1,] 3467.471</code></pre>
<p>They’re the same?!?!<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> Before you decide that at least one of the last two sections/methods was pointless, you should know that the previous example with <span class="math inline">\(LC\)</span> was a special case: we had a linear combination of the coefficients. When you have a linear combination of coefficients, the standard errors calculated by the two different methods will match. When you deviate from linear combinations of the coefficients, the methods will provide different estimates (assuming you can derive the analytical variance at all).</p>
<p>Let’s see an example.</p>
</div>
<div id="nonlinear-combinations" class="section level2">
<h2>Nonlinear combinations</h2>
<p>Suppose we have the data-generating process</p>
<p><span class="math display">\[ y_i = \beta_0 + \beta_1 x_i + \beta_2 x_2^2 + \varepsilon_i \]</span></p>
<p>OLS can handle estimating this quadratic function just fine. However, what if we want to know which value of <span class="math inline">\(x\)</span> maximizes <span class="math inline">\(y\)</span>? Assuming <span class="math inline">\(\beta_2 &gt; 0\)</span>, this function reaches its maximum at</p>
<p><span class="math display">\[ x^\text{M} = - \dfrac{\beta_1}{2 \beta_2} \]</span></p>
<p>This relationship between <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> is clearly not linear in the coefficients. Thus, while we can easily estimate <span class="math inline">\(x^\text{M}\)</span> via</p>
<p><span class="math display">\[\widehat{x}^\text{M} = - \dfrac{b_1}{2 b_2} \]</span></p>
<p>we would have a very difficult time deriving the analytical variance of this estimator. However, the Delta Method provides us with a much nicer alternative.</p>
<p>A bit more formally, in this example,</p>
<p><span class="math display">\[ \mathop{\mathbf{a}}(\boldsymbol{\beta}) = x^\text{M} = - \dfrac{\beta_1}{2 \beta_2} \]</span></p>
<p>which means</p>
<p><span class="math display">\[ \mathop{\mathbf{A}}(\boldsymbol{\beta}) =
\dfrac{\partial x^\text{M}}{\partial \boldsymbol{\beta}^\prime} =
\left[\begin{array}{ccc} \dfrac{\partial x^\text{M}}{\partial \beta_0} &amp; \dfrac{\partial x^\text{M}}{\partial \beta_1} &amp; \dfrac{\partial x^\text{M}}{\partial \beta_2} \end{array}\right] =
\left[\begin{array}{ccc} 0 &amp; - \dfrac{1}{2\beta_2} &amp; \dfrac{\beta_1}{2 \beta_2^2} \end{array}\right]
\]</span></p>
<p>Let’s bake some fake data and estimate this model. We will generate <span class="math inline">\(x\)</span> from a uniform distribution between -2 and 3; we will generate our disturbances from a normal distribution with mean zero and variance 10. We will define <span class="math inline">\(y = 4 + 4 x_i - 2 x_i^2 + \varepsilon_i\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Set the seed</span>
<span class="kw">set.seed</span>(<span class="dv">12345</span>)
<span class="co"># Set the size</span>
n &lt;-<span class="st"> </span><span class="dv">50</span>
<span class="co"># Generate data</span>
fake_df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(
  <span class="dt">x =</span> <span class="kw">runif</span>(<span class="dt">n =</span> n, <span class="dt">min =</span> -<span class="dv">2</span>, <span class="dt">max =</span> <span class="dv">3</span>),
  <span class="dt">e =</span> <span class="kw">rnorm</span>(<span class="dt">n =</span> n, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="kw">sqrt</span>(<span class="dv">10</span>))
  ) %&gt;%<span class="st"> </span><span class="kw">tbl_df</span>()
<span class="co"># Calculate y = 4 + 4x - 2x^2 + e</span>
fake_df %&lt;&gt;%<span class="st"> </span><span class="kw">mutate</span>(
  <span class="dt">x2 =</span> x^<span class="dv">2</span>,
  <span class="dt">y =</span> <span class="dv">4</span> +<span class="st"> </span><span class="dv">4</span> *<span class="st"> </span>x -<span class="st"> </span><span class="dv">2</span> *<span class="st"> </span>x^<span class="dv">2</span> +<span class="st"> </span>e)</code></pre></div>
<p>Now we calculate <span class="math inline">\(\mathbf{b}_\text{OLS}\)</span> and the variance-covariance matrix of the OLS estimator</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Estimate coefficients</span>
(b_fake &lt;-<span class="st"> </span><span class="kw">ols</span>(fake_df, <span class="st">&quot;y&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;x&quot;</span>, <span class="st">&quot;x2&quot;</span>)) %$%<span class="st"> </span>coef)</code></pre></div>
<pre><code>## [1]  4.403845  3.975403 -1.792509</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Estimate var-cov matrix</span>
v_fake &lt;-<span class="st"> </span><span class="kw">vcov_ols</span>(fake_df, <span class="st">&quot;y&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;x&quot;</span>, <span class="st">&quot;x2&quot;</span>))</code></pre></div>
<p>Next, we substitute our estimates for <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> into <span class="math inline">\(\mathop{\mathbf{A}}(\boldsymbol{\beta})\)</span> (the first-derivates matrix).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create the A matrix</span>
A_fake &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">data =</span> <span class="kw">c</span>(
  <span class="co"># The first entry of A()</span>
  <span class="dv">0</span>,
  <span class="co"># The second entry of A()</span>
  -<span class="dv">1</span>/(<span class="dv">2</span> *<span class="st"> </span>b_fake[<span class="dv">3</span>]),
  <span class="co"># The third entry of A()</span>
  b_fake[<span class="dv">2</span>]/(<span class="dv">2</span> *<span class="st"> </span>b_fake[<span class="dv">3</span>]^<span class="dv">2</span>)),
  <span class="dt">nrow =</span> <span class="dv">1</span>)</code></pre></div>
<p>Finally, we calculate <span class="math inline">\(\widehat{x}^\text{M} = \mathop{\mathbf{a}}(\mathbf{b}_\text{OLS})\)</span> and approximate<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a> its standard error, using the Delta Method as we derived above</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Our estimate for the x that maximizes y</span>
(x_m &lt;-<span class="st"> </span>-<span class="st"> </span>b_fake[<span class="dv">2</span>] /<span class="st"> </span>(<span class="dv">2</span> *<span class="st"> </span>b_fake[<span class="dv">3</span>]))</code></pre></div>
<pre><code>## [1] 1.108893</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Our estimate for the standard error</span>
(se_m &lt;-<span class="st"> </span><span class="kw">sqrt</span>(A_fake %*%<span class="st"> </span>v_fake %*%<span class="st"> </span><span class="kw">t</span>(A_fake)))</code></pre></div>
<pre><code>##           [,1]
## [1,] 0.1488887</code></pre>
<p>Let’s confirm our results using some canned functions. We can estimate the coefficients with <code>felm()</code> and then calculate the Delta-Method based standard errors using the <code>deltamethod()</code> function from the <code>msm</code> package. The <code>deltamethod()</code> function wants three things:</p>
<ol style="list-style-type: decimal">
<li><code>g</code>, a formula that relates the coefficients/parameters in terms of <code>x1</code>, <code>x2</code>, …, <em>e.g.</em>, we have have three parameters <span class="math inline">\(\beta_0,\, \beta_1\, \beta_2\)</span> which we want to relate via <span class="math inline">\(-\beta_1 / (2\beta_2)\)</span>, so our formula is <code>~ - x2 / (2 * x3)</code>. Apologies for the subscripts being off by a number.</li>
<li><code>mean</code>, our estimates for the parameters</li>
<li><code>cov</code>, the variance-covariance matrix of the parameters</li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Estimate the equation</span>
felm_fake &lt;-<span class="st"> </span><span class="kw">felm</span>(y ~<span class="st"> </span>x +<span class="st"> </span>x2, <span class="dt">data =</span> fake_df)
<span class="co"># Use the &#39;deltamethod&#39; function</span>
<span class="kw">deltamethod</span>(<span class="dt">g =</span> ~<span class="st"> </span>-<span class="st"> </span>x2 /<span class="st"> </span>(<span class="dv">2</span> *<span class="st"> </span>x3),
  <span class="dt">mean =</span> <span class="kw">coef</span>(felm_fake),
  <span class="dt">cov =</span> <span class="kw">vcov</span>(felm_fake))</code></pre></div>
<pre><code>## [1] 0.1488887</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Print the value we caluclated above</span>
se_m</code></pre></div>
<pre><code>##           [,1]
## [1,] 0.1488887</code></pre>
<p>We’re good.</p>
<p>To get a better picture of what we’ve just done, let’s plot a few things:</p>
<ul>
<li>our data</li>
<li>the predicted function, <span class="math inline">\(\hat{f}(x) = \hat{\beta_0} + \hat{\beta_1} x + \hat{\beta_2} x^2\)</span></li>
<li>the true maximum (<span class="math inline">\(x^\text{M} = 1\)</span>)</li>
<li>the predicted maximum with its 95% confidence interval</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Our plot</span>
<span class="kw">ggplot</span>(<span class="dt">data =</span> fake_df) +
<span class="st">  </span><span class="co"># 95% confidence interval for maximal x</span>
<span class="st">  </span><span class="kw">geom_rect</span>(<span class="kw">aes</span>(<span class="dt">ymin =</span> -<span class="ot">Inf</span>, <span class="dt">ymax =</span> <span class="ot">Inf</span>,
    <span class="dt">xmin =</span> x_m -<span class="st"> </span><span class="fl">1.96</span> *<span class="st"> </span>se_m, <span class="dt">xmax =</span> x_m +<span class="st"> </span><span class="fl">1.96</span> *<span class="st"> </span>se_m),
    <span class="dt">fill =</span> <span class="st">&quot;grey90&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.05</span>) +
<span class="st">  </span><span class="co"># Plot the points</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) +
<span class="st">  </span><span class="co"># Plot the predicted function</span>
<span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> function(x) {
    b_fake[<span class="dv">1</span>] +<span class="st"> </span>b_fake[<span class="dv">2</span>] *<span class="st"> </span>x +<span class="st"> </span>b_fake[<span class="dv">3</span>] *<span class="st"> </span>x^<span class="dv">2</span>
    }, <span class="dt">color =</span> <span class="st">&quot;grey65&quot;</span>) +
<span class="st">  </span><span class="co"># Vertical line at the predicted max.</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> x_m, <span class="dt">color =</span> <span class="st">&quot;grey65&quot;</span>, <span class="dt">linetype =</span> <span class="dv">2</span>) +
<span class="st">  </span><span class="co"># Vertical line at the true max.</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="dv">1</span>, <span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>) +
<span class="st">  </span><span class="co"># Title</span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Estimating the maximum of a quadratic&quot;</span>) +
<span class="st">  </span><span class="co"># Theme</span>
<span class="st">  </span>theme_ed</code></pre></div>
<p><img src="section09_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>In this figure, the solid grey line is predicted function; the solid blue vertical line shows the <span class="math inline">\(x\)</span> that sits at the true maximum; the dotted grey line gives the estimated <span class="math inline">\(x\)</span> that maximizes <span class="math inline">\(y\)</span>; and the shaded grey region gives the 95% confidence interval for this <span class="math inline">\(x\)</span> that maximizes <span class="math inline">\(y\)</span>.</p>
</div>
</div>
<div id="fun-tools-mendeley" class="section level1">
<h1>Fun tools: Mendeley</h1>
<p><a href="https://www.mendeley.com">Mendeley</a> is a (free) reference manager and viewer. I know that doesn’t sound too interesting, so let me elaborate. Mendeley allows you to read, annotate, highlight, organize, and share articles, books, and other references for your research. You can also search for existing or new papers (by author, title, year, notes…). Not only does it provide a centralized system for both your references and your notes on your references, Mendeley also allows you to assign these references to (multiple) projects—so you can have all your papers for a given topic/project linked together. Mendeley links them in a way that does not duplicate the files, so if a file exists in multiple projects, you can easily spread your notes across the projects. Mendeley also syncs across your devices, so you can read the same paper and same annotations on your tablet, laptop, phone, <em>etc.</em> Last, but not least, Mendeley will export reference documents for LaTeX, BibTeX, and Word/plain text (for a single document or for a whole project).</p>
<p>See Mendeley’s own description of its features <a href="https://www.mendeley.com/reference-management/reference-manager/">here</a>.</p>
<p><img src="Images/myMendeley.png" /> Mendeley screenshot.</p>
</div>
<div id="more" class="section level1">
<h1>More</h1>
<p>Finally, <a href="http://flowingdata.com/2015/12/15/a-day-in-the-life-of-americans/">this simulation</a> is cool.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>The reason for the quotation marks around effect is that we are not estimating <em>causal</em> effects in this regression. However, because this class (and section) is not about causal inference, we will for the moment pretend the effects are causal.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Equivalent of Stata’s <code>lincom</code>.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>While <code>estimable()</code> works with many classes of objects—<code>lm</code>, <code>glm</code>, <code>lme</code>, <code>geese</code>—it does not work with <code>felm</code> class objects. Thus we are working with <code>lm()</code> today. You can use your knowledge of the Frisch-Waugh-Lovell theorem to take care of your fixed effects and then use <code>lm()</code> for the final regression if you want to use <code>estimable()</code>… or you can just do the test yourself, since <code>felm()</code> gives you coefficient estimates and a variance-covariance matrix.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>I debated here whether the question mark should preced the exclamation mark.<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>The Delta Method provides a first-order approximation.<a href="#fnref5">↩</a></p></li>
</ol>
</div>

<!-- <?php include_once("analyticstracking.php") ?> -->

<!-- For Google Analytics: -->

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-88887510-2', 'auto');
  ga('send', 'pageview');

</script>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
